{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural PK Walkthrough\n",
    "\n",
    "This code is a rewrite of the code available at https://github.com/jameslu01/Neural_PK\n",
    "\n",
    "We have streamlined the code, and arranged its main path in this notebook.\n",
    "\n",
    "Note that the original paper and code have two different settings:\n",
    "(1) Random splits of training and test.\n",
    "(2) Cross-dosing splits. The current notebook only covers the first setting.\n",
    "\n",
    "TODO: Add a discussion of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data_split import data_split, augment_data\n",
    "from model_utils import train_neural_ode, predict_using_trained_model, merge_predictions\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# if you have a GPU and CUDA and pytorch with GPU support installed, this will come back as true\n",
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell, we set and describe the various parameters and hyperparameters that are used throghout the code.\n",
    "\n",
    "Note you'll find a few more parameters of the neural network hardcoded in model.py.\n",
    "\"\"\"\n",
    "\n",
    "BASE_RANDOM_SEED = 1329\n",
    "TORCH_RANDOM_SEED = 1000  # they have different random seeds for splitting and for the neural network\n",
    "SPLIT_FRAC = 0.2\n",
    "OUTER_FOLDS = [1, 2, 3, 4, 5]  # indices of train/test splits\n",
    "MODEL_REPLICATES = [1, 2, 3, 4, 5]  # indices of model replicates for the ensemble of neural ODEs\n",
    "\n",
    "# hyperparemeters for the model, selected by grid search\n",
    "# note: the paper was not clear WHICH hyperparameters were selected by grid search\n",
    "LR = 0.00005  # this is the most important hyperparameter to tune\n",
    "L2 = 0.1  # weight decay is a form of regularization. should be tuned\n",
    "\"\"\"\n",
    "ODE solvers can approximately ensure that the output is within a given tolerance of the true solution. \n",
    "The time spent by the forward call is proportional to the number of function evaluations, \n",
    "so tuning the tolerance gives us a trade-off between accuracy and computational cost. \n",
    "Our framework allows the user to trade off speed for precision, \n",
    "but requires the user to choose an error tolerance on both the forward and reverse passes during training. \n",
    "For sequence modeling, the default value of 1.5e-8 was used. In the classification and density estimation experiments, \n",
    "we were able to reduce the tolerance to 1e-3 and 1e-5, respectively, without degrading performance.\n",
    "In short, tol is the tolerance for accepting/rejecting an adaptive step.\n",
    "\"\"\"\n",
    "TOL = 1e-4\n",
    "# number of epochs to train the model. the authors use early stopping so it's not crucial.\n",
    "# just needs to be large enough so the val loss eventually increases\n",
    "EPOCHS = 30\n",
    "# all of these together decide the size of the neural network\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 6\n",
    "HIDDEN_DIM = 128\n",
    "ODE_HIDDEN_DIM = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example data accompanying the paper. We'll import and preprocess\n",
    "\n",
    "Data has the following columns:\n",
    "  - STUD - Study ID. Can be 1000, 2000, 3000.\n",
    "  - PTNM - Patient number. Can be repeated between studies, but for example patient 1 in Study 1000 is not the same person as patient 1 in study 2000.\n",
    "  - DSFQ - Dosage frequency is how often the dose is administred. Only 1 or 3.\n",
    "  - AMT - Dosage amount. Can be 0 when measurements taken between doses.\n",
    "  - TIME - Time since beginning of patient's treatment.\n",
    "  - TFDS - Time since dose.\n",
    "  - DV - Concentration measurement. \n",
    "\"\"\"\n",
    "data_complete = pd.read_csv(\"ExampleData/sim_data.csv\", na_values=\".\")\n",
    "\n",
    "select_cols = [\"STUD\", \"DSFQ\", \"PTNM\", \"CYCL\", \"AMT\", \"TIME\", \"TFDS\", \"DV\"]\n",
    "# According to authors: Patient data that have been marked with non-missing values in the \"C\" columns have been removed from the analysis\n",
    "if \"C\" in data_complete.columns.values:\n",
    "    data_complete = data_complete[data_complete.C.isnull()]\n",
    "data_complete = data_complete[data_complete.CYCL < 100]  # cut off all dosing cycles greater than 100\n",
    "data_complete = data_complete[select_cols]  # filter down to columns of interest\n",
    "data_complete = data_complete.rename(\n",
    "    columns={\"DV\": \"PK_timeCourse\"}\n",
    ")  # DV is our variable of interest - anolyte concentration\n",
    "data_complete[\"PTNM\"] = data_complete[\"PTNM\"].astype(\"int\").map(\"{:05d}\".format)\n",
    "data_complete[\"ID\"] = (\n",
    "    data_complete[\"STUD\"].astype(\"int\").astype(\"str\") + data_complete[\"PTNM\"]\n",
    ")  # concatenate study ID and patient ID for overall, unique ID\n",
    "\n",
    "time_summary = (\n",
    "    data_complete[[\"ID\", \"TIME\"]].groupby(\"ID\").max().reset_index()\n",
    ")  # get max time since start of treatment per ID\n",
    "# only keep patients who have measurements past initial measurements (TIME == 0)\n",
    "selected_ptnms = time_summary[time_summary.TIME > 0].ID\n",
    "data_complete = data_complete[data_complete.ID.isin(selected_ptnms)]\n",
    "\n",
    "data_complete[\"AMT\"] = data_complete[\"AMT\"].fillna(0)  # replace missing values for dosage with 0s\n",
    "\n",
    "# Set up round 1 measurement features.\n",
    "# Round 1 measurements for each ID are always used as input features for the neural network to predict measurements after round 1.\n",
    "# For weekly dosage IDs, round 1 is anything before end of week 1 (TIME <= 168), for every 3 week dosage IDs, anything before end of week 3 (TIME <= 604)\n",
    "data_complete[\"PK_round1\"] = data_complete[\"PK_timeCourse\"]\n",
    "data_complete.loc[(data_complete.DSFQ == 1) & (data_complete.TIME >= 168), \"PK_round1\"] = 0\n",
    "data_complete.loc[(data_complete.DSFQ == 3) & (data_complete.TIME >= 504), \"PK_round1\"] = 0\n",
    "\n",
    "# Missing PK measurement value handling\n",
    "data_complete[\"PK_round1\"] = data_complete[\"PK_round1\"].fillna(0)  # round 1 missing values filled with 0\n",
    "data_complete[\"PK_timeCourse\"] = data_complete[\"PK_timeCourse\"].fillna(\n",
    "    -1\n",
    ")  # all others filled with -1, used to find missing values during training\n",
    "\n",
    "data_complete = data_complete[\n",
    "    ~((data_complete.AMT == 0) & (data_complete.TIME == 0))\n",
    "]  # drop all first patient rows with no dosage\n",
    "\n",
    "# Some rows are duplicate pairs for PTNM and TIME combinations with different cycle (CYCL) values\n",
    "# Set the first dosage amount of duplicated rows to the last dosage amount and keep only last row of duplicated rows\n",
    "# This implementation may be an issue if patient number (PTNM) repeats across multiple studies (STUD)\n",
    "data_complete.loc[\n",
    "    data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"last\"), \"AMT\"  # all non-last duplicated rows\n",
    "] = data_complete.loc[\n",
    "    data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"first\"), \"AMT\"  # all non-first duplicated rows\n",
    "].values\n",
    "data = data_complete[~data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The authors of this code are doing two things when it comes to splitting the data:\n",
    "\n",
    "(1) They are doing 5 train/test splits. They repeat the entire training and test procedure 5 times.\n",
    "(2) Within each of the 5 splits above, the are also doing model averaging by training 5 Neural ODE models\n",
    "that differ in: (a) initial conditions, (b) random seeds and (c) which subset of the training data is used\n",
    "for actual model training vs validation. These 5 models are then averaged together to get the final model\n",
    "which is then applied to the test set.\n",
    "\"\"\"\n",
    "eval_results_all = {}\n",
    "for fold in OUTER_FOLDS:\n",
    "    for model in MODEL_REPLICATES:\n",
    "\n",
    "        # first we split up the data into training/validation/test\n",
    "        train, test = data_split(data, \"PTNM\", seed=BASE_RANDOM_SEED + fold, test_size=SPLIT_FRAC)\n",
    "        train, validate = data_split(train, \"PTNM\", seed=BASE_RANDOM_SEED + fold + model, test_size=SPLIT_FRAC)\n",
    "\n",
    "        \"\"\"\n",
    "        Adding the first cycle of treatment of the test set to the training set, as it will later be used\n",
    "        during test to predict the rest of the test set and not for evaluation. As such, the authors\n",
    "        think it is OK to add to training data and to maximize the total amount of training data.\n",
    "        \n",
    "        TODO: confirm that the test_add_to_train was NOT actually used in the final evaluation metrics. \n",
    "        \n",
    "        Reasoning from the paper:\n",
    "        \"Additionally, the first cycle of the observation, PK_cycle1 is also available as predictive features for the models. \n",
    "        Using the information above, we sought to predict the PK dynamics after the first cycle, i.e., \n",
    "        after 168 hr for the Q1W data and after 504 hr for the Q3W data.\"\n",
    "        \"\"\"\n",
    "        test_add_to_train = pd.concat(\n",
    "            [test[(test.DSFQ == 1) & (test.TIME < 168)], test[(test.DSFQ == 3) & (test.TIME < 504)]], ignore_index=True\n",
    "        )\n",
    "        train = pd.concat([train, test_add_to_train], ignore_index=True)\n",
    "        # i am not sure it makes sense to add this to the validation data?\n",
    "        validate = pd.concat([validate, test_add_to_train], ignore_index=True)\n",
    "\n",
    "        \"\"\"\n",
    "        They add extra data to the training set made out of existing training data. \n",
    "        Here is a description from the paper:\n",
    "\n",
    "        \"We applied augmentation to prevent overfitting.\n",
    "        We applied timewise truncation to increase the number of training examples.\n",
    "        For each training example, in addition to the original example, we also truncated\n",
    "        the examples at 1008 hr, 1512 hr, and 2016 hr and generated and added\n",
    "        a set of new examples to the training examples.\"\n",
    "        \"\"\"\n",
    "        train = augment_data(train)\n",
    "\n",
    "        # create and train the model\n",
    "        # the best checkpoint will be saved\n",
    "        train_neural_ode(\n",
    "            TORCH_RANDOM_SEED + model + fold,\n",
    "            train,\n",
    "            validate,\n",
    "            model,\n",
    "            fold,\n",
    "            LR,\n",
    "            TOL,\n",
    "            EPOCHS,\n",
    "            L2,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        # predict on test using the best model saved\n",
    "        # during train_neural_ode\n",
    "        eval_results = predict_using_trained_model(\n",
    "            test,\n",
    "            model,\n",
    "            fold,\n",
    "            TOL,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        eval_results_all[(fold, model)] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "Now we can compute evaluation metrics and summarize them\n",
    "\"\"\"\n",
    "r2_scores = []\n",
    "rmses = []\n",
    "pearsonrs = []\n",
    "for fold in OUTER_FOLDS:\n",
    "    # perform the ensembling\n",
    "    evals_per_fold = [eval_results_all[(fold, m)] for m in MODEL_REPLICATES]\n",
    "    predictions = merge_predictions(evals_per_fold, data)\n",
    "    # evaluate various metrics\n",
    "    y_true = predictions[\"labels\"].values\n",
    "    y_pred = predictions[\"pred_agg\"].values\n",
    "    rmses.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    r2_scores.append(r2_score(y_true, y_pred))\n",
    "    pearsonrs.append(pearsonr(y_true, y_pred)[0])\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"R2\": r2_scores, \"RMSE\": rmses, \"Pearson R\": pearsonrs})\n",
    "df.index = OUTER_FOLDS\n",
    "print(df)\n",
    "\n",
    "summary_df = df.agg([\"min\", \"max\", \"mean\", \"median\"])\n",
    "print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
