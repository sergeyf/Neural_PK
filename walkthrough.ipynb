{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural PK Walkthrough\n",
    "\n",
    "This code is a rewrite of the code available at https://github.com/jameslu01/Neural_PK\n",
    "\n",
    "We have streamlined the code, and arranged its main path in this notebook.\n",
    "\n",
    "Note that the original paper and code have two different settings:\n",
    "(1) Random splits of training and test.\n",
    "(2) Cross-dosing splits. The current notebook only covers the first setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import pandas as pd\n",
    "from data_split import data_split, data_split_cross, augment_data\n",
    "from model_utils import train_neural_ode, predict_using_trained_model, merge_predictions\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from scipy.stats import pearsonr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you have a GPU and CUDA and pytorch with GPU support installed, this will come back as true\n",
    "# it's ok if you don't have CUDA - you can still run all of the model code, but it will be slower\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "In this cell, we set and describe the various parameters and hyperparameters that are used throghout the code.\n",
    "\n",
    "Note you'll find a few more parameters of the neural network hardcoded in model.py.\n",
    "\"\"\"\n",
    "\n",
    "BASE_RANDOM_SEED = 1329  # random seed for data splitting\n",
    "TORCH_RANDOM_SEED = 1000  # they have different random seeds for splitting and for the neural network\n",
    "SPLIT_FRAC = 0.2  # fraction of data reserved for testing\n",
    "OUTER_FOLDS = [1, 2, 3, 4, 5]  # indices of train/test splits\n",
    "CROSS_SCHED_FOLD = [999] # single fold for the cross-schedule tran/test split - used for setting seed and naming files\n",
    "MODEL_REPLICATES = [1, 2, 3, 4, 5]  # indices of model replicates for the ensemble of neural ODEs\n",
    "\n",
    "# hyperparemeters for the model, selected by grid search\n",
    "# note: the paper was not clear WHICH hyperparameters were selected by grid search\n",
    "LR = 0.00005  # this is the most important hyperparameter to tune\n",
    "L2 = 0.1  # weight decay is a form of regularization. should be tuned\n",
    "\"\"\"\n",
    "ODE solvers can approximately ensure that the output is within a given tolerance of the true solution. \n",
    "The time spent by the forward call is proportional to the number of function evaluations, \n",
    "so tuning the tolerance gives us a trade-off between accuracy and computational cost. \n",
    "Our framework allows the user to trade off speed for precision, \n",
    "but requires the user to choose an error tolerance on both the forward and reverse passes during training. \n",
    "For sequence modeling, the default value of 1.5e-8 was used. In the classification and density estimation experiments, \n",
    "we were able to reduce the tolerance to 1e-3 and 1e-5, respectively, without degrading performance.\n",
    "In short, tol is the tolerance for accepting/rejecting an adaptive step.\n",
    "\"\"\"\n",
    "TOL = 1e-4\n",
    "# number of epochs to train the model. the authors use early stopping so it's not crucial.\n",
    "# just needs to be large enough so the validation loss eventually decreases\n",
    "EPOCHS = 30\n",
    "# all of these together decide the size of the neural network\n",
    "HIDDEN_DIM = 128\n",
    "LATENT_DIM = 6\n",
    "ODE_HIDDEN_DIM = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and Processing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Example data accompanying the paper. We'll import and preprocess in this cell.\n",
    "\n",
    "Data has the following columns:\n",
    "  - STUD - Study ID. Can be 1000, 2000, 3000.\n",
    "  - PTNM - Patient number. Can be repeated between studies, but for example patient 1 in Study 1000 is not the same person as patient 1 in study 2000.\n",
    "  - DSFQ - Dosage frequency is how often the dose is administred. Only 1 or 3.\n",
    "  - AMT - Dosage amount. Can be 0 when measurements taken between doses.\n",
    "  - TIME - Time since beginning of patient's treatment.\n",
    "  - TFDS - Time since dose.\n",
    "  - DV - Concentration measurement. \n",
    "\"\"\"\n",
    "data_complete = pd.read_csv(\"ExampleData/sim_data.csv\", na_values=\".\")\n",
    "\n",
    "select_cols = [\"STUD\", \"DSFQ\", \"PTNM\", \"CYCL\", \"AMT\", \"TIME\", \"TFDS\", \"DV\"]\n",
    "# According to authors: Patient data that have been marked with non-missing values in the \"C\" columns have been removed from the analysis\n",
    "if \"C\" in data_complete.columns.values:\n",
    "    data_complete = data_complete[data_complete.C.isnull()]\n",
    "data_complete = data_complete[data_complete.CYCL < 100]  # cut off all dosing cycles greater than 100\n",
    "data_complete = data_complete[select_cols]  # filter down to columns of interest\n",
    "data_complete = data_complete.rename(\n",
    "    columns={\"DV\": \"PK_timeCourse\"}\n",
    ")  # DV is our variable of interest - anolyte concentration\n",
    "data_complete[\"PTNM\"] = data_complete[\"PTNM\"].astype(\"int\").map(\"{:05d}\".format)\n",
    "data_complete[\"ID\"] = (\n",
    "    data_complete[\"STUD\"].astype(\"int\").astype(\"str\") + data_complete[\"PTNM\"]\n",
    ")  # concatenate study ID and patient ID for overall, unique ID\n",
    "\n",
    "time_summary = (\n",
    "    data_complete[[\"ID\", \"TIME\"]].groupby(\"ID\").max().reset_index()\n",
    ")  # get max time since start of treatment per ID\n",
    "# only keep patients who have measurements past initial measurements (TIME == 0)\n",
    "selected_ptnms = time_summary[time_summary.TIME > 0].ID\n",
    "data_complete = data_complete[data_complete.ID.isin(selected_ptnms)]\n",
    "\n",
    "data_complete[\"AMT\"] = data_complete[\"AMT\"].fillna(0)  # replace missing values for dosage with 0s\n",
    "\n",
    "# Set up round 1 measurement features.\n",
    "# Round 1 measurements for each ID are always used as input features for the neural network to predict measurements after round 1.\n",
    "# For weekly dosage IDs, round 1 is anything before end of week 1 (TIME <= 168), for every 3 week dosage IDs, anything before end of week 3 (TIME <= 604)\n",
    "data_complete[\"PK_round1\"] = data_complete[\"PK_timeCourse\"]\n",
    "data_complete.loc[(data_complete.DSFQ == 1) & (data_complete.TIME >= 168), \"PK_round1\"] = 0\n",
    "data_complete.loc[(data_complete.DSFQ == 3) & (data_complete.TIME >= 504), \"PK_round1\"] = 0\n",
    "\n",
    "# Missing PK measurement value handling\n",
    "data_complete[\"PK_round1\"] = data_complete[\"PK_round1\"].fillna(0)  # round 1 missing values filled with 0\n",
    "data_complete[\"PK_timeCourse\"] = data_complete[\"PK_timeCourse\"].fillna(\n",
    "    -1\n",
    ")  # all others filled with -1, used to find missing values during training\n",
    "\n",
    "data_complete = data_complete[\n",
    "    ~((data_complete.AMT == 0) & (data_complete.TIME == 0))\n",
    "]  # drop all first patient rows with no dosage\n",
    "\n",
    "# Some rows are duplicate pairs for PTNM and TIME combinations with different cycle (CYCL) values\n",
    "# Set the first dosage amount of duplicated rows to the last dosage amount and keep only last row of duplicated rows\n",
    "# This implementation may be an issue if patient number (PTNM) repeats across multiple studies (STUD)\n",
    "data_complete.loc[\n",
    "    data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"last\"), \"AMT\"  # all non-last duplicated rows\n",
    "] = data_complete.loc[\n",
    "    data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"first\"), \"AMT\"  # all non-first duplicated rows\n",
    "].values\n",
    "data = data_complete[~data_complete[[\"PTNM\", \"TIME\"]].duplicated(keep=\"first\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random (Non-Cross Schedule) Prediction W/ 5-Fold CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here the data is split, the model is trained, and then evaluated on test data. The raw results\n",
    "from this evaluatoin are saved.\n",
    "\n",
    "This entire procedure is done multiple times due to multiple training/test splits and model averaging. \n",
    "The authors of this code are doing two things when it comes to splitting the data:\n",
    "\n",
    "(1) They are doing 5 outer train/test splits. They repeat the entire training and test procedure 5 times.\n",
    "This is good practice to establish variance of the test set performance.\n",
    "(2) Within each of the 5 splits above, the are also doing model *averaging* by training 5 Neural ODE models\n",
    "that differ in: (a) initial conditions, (b) random seeds and (c) which subset of the training data is used\n",
    "for actual model training vs validation. These 5 models are then averaged together to get the final model\n",
    "which is then applied to the test set. This is standard practice to improve model quality. It also brings up\n",
    "an interesting methodological question: did they also use 5 model replicates for the other algorithms\n",
    "that they benchmarked against? If not, then their model had an unfair advantage.\n",
    "\n",
    "Feel free to skip this cell: we've already run the code and stored the results in CSVs.\n",
    "\"\"\"\n",
    "eval_results_all = {}\n",
    "for fold in OUTER_FOLDS:\n",
    "    for model in MODEL_REPLICATES:\n",
    "\n",
    "        # first we split up the data into training/validation/test\n",
    "        train, test = data_split(data, \"PTNM\", seed=BASE_RANDOM_SEED + fold, test_size=SPLIT_FRAC)\n",
    "        train, validate = data_split(train, \"PTNM\", seed=BASE_RANDOM_SEED + fold + model, test_size=SPLIT_FRAC)\n",
    "\n",
    "        \"\"\"\n",
    "        We don't know why this is happening. We asked the authors but they did not respond. \n",
    "        They are adding the first week of the test data to the training and validation data.\n",
    "        Although this data is not used for evaluation (see the last few lines of merge_predictions in model_utils.py), \n",
    "        it is used for training the model. Our best guess is that this provides a bit of extra training data,\n",
    "        but it's unrealistic as we won't have the first part of \"real\" test data in practice.\n",
    "        \"\"\"\n",
    "        test_add_to_train = pd.concat(\n",
    "            [test[(test.DSFQ == 1) & (test.TIME < 168)], test[(test.DSFQ == 3) & (test.TIME < 504)]], ignore_index=True\n",
    "        )\n",
    "        train = pd.concat([train, test_add_to_train], ignore_index=True)\n",
    "        # i am not sure it makes sense to add this to the validation data?\n",
    "        validate = pd.concat([validate, test_add_to_train], ignore_index=True)\n",
    "\n",
    "        \"\"\"\n",
    "        They add extra augmented data to the training set constructed out of existing training data. \n",
    "        Here is a description from the paper:\n",
    "\n",
    "        \"We applied augmentation to prevent overfitting.\n",
    "        We applied timewise truncation to increase the number of training examples.\n",
    "        For each training example, in addition to the original example, we also truncated\n",
    "        the examples at 1008 hr, 1512 hr, and 2016 hr and generated and added\n",
    "        a set of new examples to the training examples.\"\n",
    "        \"\"\"\n",
    "        train = augment_data(train)\n",
    "\n",
    "        # create and train the model\n",
    "        # the best checkpoint will be saved\n",
    "        train_neural_ode(\n",
    "            TORCH_RANDOM_SEED + model + fold,\n",
    "            train,\n",
    "            validate,\n",
    "            model,\n",
    "            fold,\n",
    "            LR,\n",
    "            TOL,\n",
    "            EPOCHS,\n",
    "            L2,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        # predict on test using the best model saved\n",
    "        # during train_neural_ode\n",
    "        eval_results = predict_using_trained_model(\n",
    "            test,\n",
    "            model,\n",
    "            fold,\n",
    "            TOL,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        eval_results_all[(fold, model)] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to do this because we have eval_results_all from the previous cell, \n",
    "# but just a quick demonstration of how to load the predictions from the saved models\n",
    "eval_results_all_loaded = {}\n",
    "\n",
    "for fold in OUTER_FOLDS:\n",
    "    for model in MODEL_REPLICATES:\n",
    "        eval_path = os.path.join(f\"fold_{fold}\", f\"fold_{fold}_model_{model}.csv\")\n",
    "        eval_results_all_loaded[(fold, model)] = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can compute evaluation metrics and summarize them\n",
    "\n",
    "The paper discusses two of the metrics:\n",
    "\n",
    "\"We used R2 score, and Pearson correlation in this study. \n",
    "Correlation gives an intuitive estimation of the concordance between the predictions and ground truth,\n",
    "while R2 also takes into account whether the overall scale of the predictions is correct.\"\n",
    "\n",
    "However, the code also uses an additional metrics of root mean squared error (RMSE), which is a \n",
    "standard regression metric. It provides an additional view of the actual distance between the predictions\n",
    "which is not available in R2 or correlation.\n",
    "\n",
    "We have also added a new metric of the absolute difference between the predictions and the ground truth\n",
    "because we believe it is a useful viewpoint that does not have the \"squared\" distortion of the RMSE.\n",
    "\"\"\"\n",
    "r2_scores = []\n",
    "rmses = []\n",
    "pearsonrs = []\n",
    "abs_errors = []\n",
    "for fold in OUTER_FOLDS:\n",
    "    # perform the ensembling\n",
    "    evals_per_fold = [eval_results_all_loaded[(fold, m)] for m in MODEL_REPLICATES]\n",
    "    predictions = merge_predictions(evals_per_fold, data)\n",
    "    # evaluate various metrics\n",
    "    y_true = predictions[\"labels\"].values\n",
    "    y_pred = predictions[\"pred_agg\"].values\n",
    "    rmses.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    r2_scores.append(r2_score(y_true, y_pred))\n",
    "    pearsonrs.append(pearsonr(y_true, y_pred)[0])\n",
    "    abs_errors.append(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"R2\": r2_scores, \"Pearson R\": pearsonrs, \"RMSE\": rmses, \"Mean Absolute Error\": abs_errors})\n",
    "df.index = OUTER_FOLDS\n",
    "print(df, '\\n')\n",
    "\n",
    "summary_df = df.agg([\"min\", \"max\", \"mean\", \"median\"])\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Schedule Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Using the same building blocks, we can also train a model to predict cross-schedule PK response.\n",
    "\n",
    "In this case, we only run one outer fold, but still train 5 models within this fold. Each of the \n",
    "models has the same test split (train and validate on 3-week dosing schedule, test on 1-week dosing \n",
    "schedule), but different train/validation splits (split randomly).\n",
    "\n",
    "In this code, we have implemented the training and prediction of the split described above. The\n",
    "original code creates 2 additional test sets:\n",
    "(1) test_ineterp - the same test set, but with interpolated time points at 6 hour intervals. This was\n",
    "    done to be able to generate predictions at a higher resolution.\n",
    "(2) test_nodosing - the same test set with all dosing information masked (replaced with 0). This was\n",
    "    used to produce the analysis described in Figure 4B.\n",
    "\"\"\"\n",
    "eval_results_all = {}\n",
    "# we retained the outer fold loop for similarity of code. This can be replaced with \n",
    "# fold = CROSS_SCHED_FOLD, as it is only used for setting the seed and naming files\n",
    "for fold in CROSS_SCHED_FOLD:\n",
    "    for model in MODEL_REPLICATES:\n",
    "\n",
    "        # first we split up the data into training/validation/test\n",
    "        # in this case, we split 1-week schedule patients to test,\n",
    "        # then split the into train and validate randomly as above\n",
    "        train, test = data_split_cross(data)\n",
    "        train, validate = data_split(train, \"PTNM\", seed=BASE_RANDOM_SEED + fold + model, test_size=SPLIT_FRAC)\n",
    "\n",
    "        # note that we do not add any of the test data to the train\n",
    "        # and validate data as we did above\n",
    "\n",
    "        # the training data is augmented identically as above\n",
    "        train = augment_data(train)\n",
    "\n",
    "        # create and train the model\n",
    "        # the best checkpoint will be saved\n",
    "        train_neural_ode(\n",
    "            TORCH_RANDOM_SEED + model + fold,\n",
    "            train,\n",
    "            validate,\n",
    "            model,\n",
    "            fold,\n",
    "            LR,\n",
    "            TOL,\n",
    "            EPOCHS,\n",
    "            L2,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        # predict on test using the best model saved\n",
    "        # during train_neural_ode\n",
    "        eval_results = predict_using_trained_model(\n",
    "            test,\n",
    "            model,\n",
    "            fold,\n",
    "            TOL,\n",
    "            HIDDEN_DIM,\n",
    "            LATENT_DIM,\n",
    "            ODE_HIDDEN_DIM,\n",
    "        )\n",
    "\n",
    "        eval_results_all[(fold, model)] = eval_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# No need to do this because we have eval_results_all from the previous cell, \n",
    "# but just a quick demonstration of how to load the predictions from the saved models\n",
    "eval_results_all_loaded = {}\n",
    "\n",
    "for fold in CROSS_SCHED_FOLD:\n",
    "    for model in MODEL_REPLICATES:\n",
    "        eval_path = os.path.join(f\"fold_{fold}\", f\"fold_{fold}_model_{model}.csv\")\n",
    "        eval_results_all_loaded[(fold, model)] = pd.read_csv(eval_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Now we can compute evaluation metrics same as before.\n",
    "\"\"\"\n",
    "r2_scores = []\n",
    "rmses = []\n",
    "pearsonrs = []\n",
    "abs_errors = []\n",
    "for fold in CROSS_SCHED_FOLD:\n",
    "    # perform the ensembling\n",
    "    evals_per_fold = [eval_results_all[(fold, m)] for m in MODEL_REPLICATES]\n",
    "    predictions = merge_predictions(evals_per_fold, data)\n",
    "    # evaluate various metrics\n",
    "    y_true = predictions[\"labels\"].values\n",
    "    y_pred = predictions[\"pred_agg\"].values\n",
    "    rmses.append(mean_squared_error(y_true, y_pred, squared=False))\n",
    "    r2_scores.append(r2_score(y_true, y_pred))\n",
    "    pearsonrs.append(pearsonr(y_true, y_pred)[0])\n",
    "    abs_errors.append(mean_absolute_error(y_true, y_pred))\n",
    "\n",
    "\n",
    "df = pd.DataFrame({\"R2\": r2_scores, \"Pearson R\": pearsonrs, \"RMSE\": rmses, \"Mean Absolute Error\": abs_errors})\n",
    "df.index = CROSS_SCHED_FOLD\n",
    "print(df)\n",
    "\n",
    "# summary_df is not necessary if we are not running multuple folds\n",
    "# summary_df = df.agg([\"min\", \"max\", \"mean\", \"median\"])\n",
    "# print(summary_df)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2be5faf79681da6f2a61fdfdd5405d65d042280f7fba6178067603e3a2925119"
  },
  "kernelspec": {
   "display_name": "Python 3.10.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
